{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-CLIPS: A conceptual introduction to Cooperative Language-Guided Inverse Plan Search (CLIPS)\n",
    "\n",
    "This tutorial will introduce you to (a miniature version of) CLIPS, a Bayesian\n",
    "framework for grounded instruction following and goal assistance that accounts\n",
    "for pragmatic context. Using CLIPS, you can build AI assistants that:\n",
    "\n",
    "- Infer a user's goals and commands from their actions and instructions.\n",
    "- Reliably ground inferred commands into executable sequences of actions.\n",
    "- Interpret ambiguous instructions by taking into account the user's goals and actions.\n",
    "- Maintain uncertainty over the user's goals if there is insufficient information.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. [Environment setup](#setup)\n",
    "2. [Defining a Bayesian user model](#defining-model)\n",
    "3. [Inferring goals from user actions](#goals-from-actions)\n",
    "4. [Modeling natural language instructions with LLMs](#modeling-instructions)\n",
    "5. [Inferring goals from actions and instructions](#goals-from-instructions)\n",
    "6. [Interactive user assistance](#user-assistance)\n",
    "7. [Possible extensions](#extensions)\n",
    "\n",
    "## 1. Environment setup <a name=\"setup\"></a>\n",
    "\n",
    "Since CLIPS is a Bayesian framework that uses large language models (LLMs) to\n",
    "model how people communicate instructions in language, we're going to use the\n",
    "[Gen.jl](https://www.gen.dev/) probabilistic programming system to implement\n",
    "Bayesian models, and the [GenGPT3.jl](https://github.com/probcomp/GenGPT3.jl)\n",
    "library to query OpenAI's LLMs for completions and their probabilities.\n",
    "\n",
    "In this Colab notebook, we're first going to install Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "set -e\n",
    "# Adapted from https://github.com/ageron/julia_notebooks\n",
    "#---------------------------------------------------#\n",
    "JULIA_VERSION=\"1.10.2\" # any version ≥ 0.7.0\n",
    "JULIA_PACKAGES=\"IJulia BenchmarkTools\"\n",
    "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
    "JULIA_NUM_THREADS=2\n",
    "#---------------------------------------------------#\n",
    "\n",
    "if [ -z `which julia` ]; then\n",
    "  # Install Julia\n",
    "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
    "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
    "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
    "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
    "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
    "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
    "  rm /tmp/julia.tar.gz\n",
    "\n",
    "  # Install Packages\n",
    "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
    "  if [ $GPU -eq 1 ]; then\n",
    "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
    "  fi\n",
    "  for PKG in `echo $JULIA_PACKAGES`; do\n",
    "    echo \"Installing Julia package $PKG...\"\n",
    "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
    "  done\n",
    "\n",
    "  # Install kernel and rename it to \"julia\"\n",
    "  echo \"Installing IJulia kernel...\"\n",
    "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
    "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
    "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
    "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
    "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
    "\n",
    "  echo ''\n",
    "  echo \"Successfully installed `julia -v`!\"\n",
    "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key)\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above, reload the page, then change the runtime type to Julia by selecting *Runtime > Change runtime type* in the menu.\n",
    "\n",
    "We can now install the necessary packages using Julia's package manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"Gen\", \"IterTools\"])\n",
    "Pkg.add(url=\"https://github.com/probcomp/GenGPT3.jl.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load Gen, GenGPT3.jl, and other utilities into our current workspace.\n",
    "(Note that precompiling packages may take some time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen, GenGPT3\n",
    "using Printf, Random, IterTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make sure your OpenAI API key is set as an environment variable. You can\n",
    "do so by [following this guide](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety),\n",
    "or by manually setting the value of `ENV[\"OPENAI_API_KEY\"]` in the cell below.\n",
    "However, it is *not* recommended to save your API key in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# ENV[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining a Bayesian user model <a name=\"defining-model\"></a>\n",
    "\n",
    "Using Gen, we can define a Bayesian model of a user who interacts with the\n",
    "environment in a structured way:\n",
    "1. First, the user decides on a goal $g$.\n",
    "2. Next, they come up with a plan (or policy) $\\pi$ to achieve the goal $g$.\n",
    "3. Then at each step $t$, the user eithers take an action $a_t$ by following the\n",
    "   plan, or communicate parts of their plan as an instruction $u_t$.\n",
    "\n",
    "Our AI assistant gets to observe $a_t$ and $u_t$ at each step $t$, and can\n",
    "choose to take actions in response. As an example, let's consider a grocery\n",
    "store as our environment, where the user's goal is to acquire ingredients for\n",
    "one of four possible recipes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of possible goals\n",
    "GOALS = [\n",
    "    \"greek_salad\",\n",
    "    \"veggie_burger\",\n",
    "    \"fried_rice\",\n",
    "    \"burrito_bowl\"\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the full version of CLIPS, goals are defined as conjunctions of predicates\n",
    "(i.e. facts about the environment) that the user wants to achieve. A planning\n",
    "algorithm then *automatically* derives a plan or policy $\\pi$ that the user\n",
    "might follow to their goal $g$. For simplicity, we'll instead *manually* define\n",
    "a (partially ordered) plan to each goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plans to each goal\n",
    "PLANS = Dict(\n",
    "   \"greek_salad\" => Dict(\n",
    "        \"get(tomato)\" => String[],\n",
    "        \"get(olives)\" => String[],\n",
    "        \"get(cucumber)\" => String[],\n",
    "        \"get(onion)\" => String[],\n",
    "        \"get(feta_cheese)\" => String[],\n",
    "        \"checkout()\" => String[\n",
    "            \"get(tomato)\", \"get(olives)\", \"get(cucumber)\",\n",
    "            \"get(onion)\", \"get(feta_cheese)\"\n",
    "        ]\n",
    "   ),\n",
    "   \"veggie_burger\" => Dict(\n",
    "        \"get(hamburger_bun)\" => String[],\n",
    "        \"get(tomato)\" => String[],\n",
    "        \"get(onion)\" => String[],\n",
    "        \"get(lettuce)\" => String[],\n",
    "        \"get(frozen_patty)\" => String[\n",
    "            \"get(hamburger_bun)\", \"get(tomato)\",\n",
    "            \"get(onion)\", \"get(lettuce)\"\n",
    "        ],\n",
    "        \"checkout()\" => String[\n",
    "            \"get(hamburger_bun)\", \"get(tomato)\", \"get(onion)\",\n",
    "            \"get(lettuce)\", \"get(frozen_patty)\"\n",
    "        ]\n",
    "   ),\n",
    "   \"fried_rice\" => Dict(\n",
    "        \"get(rice)\" => String[],\n",
    "        \"get(onion)\" => String[],\n",
    "        \"get(soy_sauce)\" => String[],\n",
    "        \"get(frozen_peas)\" => String[\n",
    "            \"get(rice)\", \"get(onion)\", \"get(soy_sauce)\"\n",
    "        ],\n",
    "        \"get(frozen_carrots)\" => String[\n",
    "            \"get(rice)\", \"get(onion)\", \"get(soy_sauce)\"\n",
    "        ],\n",
    "        \"checkout()\" => String[\n",
    "            \"get(rice)\", \"get(onion)\", \"get(soy_sauce)\",\n",
    "            \"get(frozen_peas)\", \"get(frozen_carrots)\"\n",
    "        ]\n",
    "   ),\n",
    "   \"burrito_bowl\" => Dict(\n",
    "        \"get(rice)\" => String[],\n",
    "        \"get(black_beans)\" => String[],\n",
    "        \"get(cotija_cheese)\" => String[],\n",
    "        \"get(onion)\" => String[],\n",
    "        \"get(tomato)\" => String[],\n",
    "        \"checkout()\" => String[\n",
    "            \"get(rice)\", \"get(black_beans)\", \"get(cotija_cheese)\",\n",
    "            \"get(onion)\", \"get(tomato)\"\n",
    "        ]\n",
    "   )\n",
    ");\n",
    "\n",
    "# Set of possible actions\n",
    "ACTIONS = sort!(collect(union((keys(plan) for plan in values(PLANS))...)))\n",
    "push!(ACTIONS, \"wait()\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each partially ordered plan corresponds to a set of actions that must be\n",
    "performed to achieve the goal, along with dependencies between those actions.\n",
    "For example, the `checkout()` action has to be performed after all other\n",
    "actions. We also assume that the user plans to collect frozen food items only\n",
    "after acquiring all non-frozen items.\n",
    "\n",
    "With our goals and plans defined, we can now model how a user takes actions\n",
    "by writing a probabilistic program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act_only_user_model"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Labeled uniform distribution.\"\n",
    "@dist labeled_uniform(labels) = labels[uniform_discrete(1, length(labels))]\n",
    "\n",
    "\"Labeled categorical distribution.\"\n",
    "@dist labeled_categorical(labels, probs) = labels[categorical(probs)]\n",
    "\n",
    "\"Returns the actions the user might execute at a `state` given their `plan`.\"\n",
    "function get_planned_actions(state::Set, plan::Dict)\n",
    "    planned_acts = filter(collect(keys(plan))) do act\n",
    "        if act in state\n",
    "            return false # Filter out completed actions\n",
    "        elseif !all(act_dep in state for act_dep in plan[act])\n",
    "            return false # Filter out actions with unfulfilled dependencies\n",
    "        else\n",
    "            return true # Keep actions that are planned and not completed\n",
    "        end\n",
    "    end\n",
    "    sort!(planned_acts)\n",
    "    if isempty(planned_acts)\n",
    "        push!(planned_acts, \"wait()\")\n",
    "    end\n",
    "    return planned_acts\n",
    "end\n",
    "\n",
    "\"Model of user's goal-directed actions over time.\"\n",
    "@gen function act_only_user_model(T::Int, act_noise::Real = 0.05)\n",
    "    # Construct initial state of environment\n",
    "    state = Set{String}()\n",
    "    # Sample user's goal and select plan\n",
    "    goal ~ labeled_uniform(GOALS)\n",
    "    plan = PLANS[goal]\n",
    "    # Sample actions at each timestep\n",
    "    act_history = String[]\n",
    "    for t in 1:T\n",
    "        # Determine user's next possible actions\n",
    "        planned_acts = get_planned_actions(state, plan)\n",
    "        planned_probs = fill((1.0 - act_noise) / length(planned_acts),\n",
    "                             length(planned_acts))\n",
    "        # Determine set of unexecuted actions\n",
    "        possible_acts = filter(!in(state), ACTIONS)\n",
    "        possible_probs = fill(act_noise / length(possible_acts),\n",
    "                              length(possible_acts))\n",
    "        # Sample next action (with some action noise)\n",
    "        next_acts = vcat(planned_acts, possible_acts)\n",
    "        next_act_probs = vcat(planned_probs, possible_probs)\n",
    "        act = {(:act, t)} ~ labeled_categorical(next_acts, next_act_probs)\n",
    "        # Update state and action history\n",
    "        if act != \"wait()\"\n",
    "            push!(state, act)\n",
    "        end\n",
    "        push!(act_history, act)\n",
    "    end\n",
    "    # Return final state and action history\n",
    "    return (state, act_history)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program, we first sample the user's `goal` from a uniform prior over\n",
    "goals. We then select the `plan` that corresponds to that goal, and\n",
    "simulate how the user might act in accordance with that plan for `T` steps.\n",
    "\n",
    "Specifically, we assume that with probability `(1 - act_noise)`, the user\n",
    "selects one of their planned actions. Otherwise, the user makes a \"mistake\",\n",
    "executing a random action. After taking an action, we update the environment\n",
    "`state` to keep track of all actions that have been achieved.\n",
    "\n",
    "Let's sample an execution trace from this model, and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal = \"burrito_bowl\"\n",
      "act_history = [\"get(rice)\", \"get(onion)\", \"get(black_beans)\", \"get(tomato)\", \"get(cotija_cheese)\", \"checkout()\"]\n",
      "state = Set([\"get(rice)\", \"get(tomato)\", \"get(onion)\", \"get(black_beans)\", \"checkout()\", \"get(cotija_cheese)\"])\n"
     ]
    }
   ],
   "source": [
    "T, act_noise = 6, 0.0\n",
    "trace = Gen.simulate(act_only_user_model, (T, act_noise))\n",
    "goal = trace[:goal]\n",
    "state, act_history = get_retval(trace)\n",
    "@show goal;\n",
    "@show act_history;\n",
    "@show state;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the simulated actions correspond with the sampled goal. The\n",
    "order of the actions also respects the constraints imposed by the partially\n",
    "ordered plan.\n",
    "\n",
    "With Gen, we aren't limited to simulating possible traces from a probabilistic\n",
    "program. We can also *evaluate* the probability that a particular trace was\n",
    "generated by our program. We do this by specifying a **choice map**: a mapping\n",
    "of random variables to specific values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_trace = 0.0041666666666666675\n"
     ]
    }
   ],
   "source": [
    "# Specify values of random choices\n",
    "constraints = choicemap(\n",
    "    (:goal, \"greek_salad\"),\n",
    "    ((:act, 1), \"get(tomato)\"),\n",
    "    ((:act, 2), \"get(onion)\"),\n",
    "    ((:act, 3), \"get(olives)\"),\n",
    ")\n",
    "\n",
    "# Evaluate the probability of a trace with those choices\n",
    "T, act_noise = 3, 0.0\n",
    "trace, _ = Gen.generate(act_only_user_model, (T, act_noise), constraints)\n",
    "p_trace = exp(Gen.get_score(trace))\n",
    "\n",
    "# P(trace) = P(goal) P(actions | goal) = 1/4 * (1/5 * 1/4 * 1/3) ≈ 0.004167\n",
    "@show p_trace;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you manually work out the probability of the trace, you'll see that it's\n",
    "equal to the value that Gen automatically calculated for us.\n",
    "\n",
    "## 3. Inferring goals from user actions <a name=\"goals-from-actions\"></a>\n",
    "\n",
    "Since we can evaluate the probability of each trace, we can use this to\n",
    "implement an enumerative Bayesian inference algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enum_inference"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    enum_inference(model, model_args, observations, latent_addrs, latent_values)\n",
    "\n",
    "Runs enumerative Bayesian inference for a `model` parameterized by `model_args`,\n",
    "conditioned on the `observations`. Given a list of `latent_addrs` and the\n",
    "a list of corresponding `latent_values` that each latent variable can take on,\n",
    "we enumerate over all possible settings of the latent variables.\n",
    "\n",
    "Returns a named tuple with the following fields:\n",
    "- `traces`: An array of execution traces for each combination of latent values.\n",
    "- `logprobs`: An array of log probabilities for each trace.\n",
    "- `latent_logprobs`: A dictionary of log posterior probabilities per latent.\n",
    "- `latent_probs`: A dictionary of posterior probabilities per latent.\n",
    "- `lml`: The log marginal likelihood of the observations.\n",
    "\"\"\"\n",
    "function enum_inference(\n",
    "    model::GenerativeFunction, model_args::Tuple,\n",
    "    observations::ChoiceMap, latent_addrs, latent_values\n",
    ")\n",
    "    @assert length(latent_addrs) == length(latent_values)\n",
    "    # Construct iterator over combinations of latent values\n",
    "    latents_iter = Iterators.product(latent_values...)\n",
    "    # Generate a trace for each possible combination of latent values\n",
    "    traces = map(latents_iter) do latents\n",
    "        constraints = choicemap()\n",
    "        for (addr, val) in zip(latent_addrs, latents)\n",
    "            constraints[addr] = val\n",
    "        end\n",
    "        constraints = merge(constraints, observations)\n",
    "        tr, _ = Gen.generate(model, model_args, constraints)\n",
    "        return tr\n",
    "    end\n",
    "    # Compute the log probability of each trace\n",
    "    logprobs = map(Gen.get_score, traces)\n",
    "    # Compute the log marginal likelihood of the observations\n",
    "    lml = logsumexp(logprobs)\n",
    "    # Compute the (marginal) posterior probabilities for each latent variable\n",
    "    latent_logprobs = Dict(\n",
    "        addr => ([logsumexp(lps) for lps in eachslice(logprobs, dims=i)] .- lml)\n",
    "        for (i, addr) in enumerate(latent_addrs)\n",
    "    )\n",
    "    latent_probs = Dict(addr => exp.(lp) for (addr, lp) in latent_logprobs)\n",
    "    return (\n",
    "        traces = traces,\n",
    "        logprobs = logprobs,\n",
    "        latent_logprobs = latent_logprobs,\n",
    "        latent_probs = latent_probs,\n",
    "        latent_addrs = latent_addrs,\n",
    "        lml = lml\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this algorithm on a sequence of observed actions $a_{1:t}$, we can\n",
    "infer the goal posterior $P(g | a_{1:t})$ given a sequence of actions:\n",
    "\n",
    "$$P(g | a_{1:t}) = \\frac{P(g, a_{1:t})}{P(a_{1:t})}$$\n",
    "\n",
    "Let's see what happens when we observe that the user gets a tomato, followed\n",
    "by an onion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(goal)\t\tgoal\n",
      "0.272\t\tgreek_salad\n",
      "0.450\t\tveggie_burger\n",
      "0.006\t\tfried_rice\n",
      "0.272\t\tburrito_bowl\n"
     ]
    }
   ],
   "source": [
    "# Observed actions\n",
    "observations = choicemap(\n",
    "    ((:act, 1), \"get(tomato)\"),\n",
    "    ((:act, 2), \"get(onion)\"),\n",
    ")\n",
    "\n",
    "# Run inference by enumerating over all possible goals\n",
    "T = 2\n",
    "results = enum_inference(\n",
    "    act_only_user_model, (T,), observations, (:goal,), (GOALS,)\n",
    ")\n",
    "\n",
    "# Show inferred goal probabilities\n",
    "println(\"P(goal)\\t\\tgoal\")\n",
    "for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inferred distribution shows that user the might be collecting ingredients\n",
    "for Greek salad, a veggie burger, or a burrito_bowl. This makes sense, since\n",
    "only those recipes require both an onion and tomato. In contrast, fried rice\n",
    "is very unlikely to be the user's goal.\n",
    "\n",
    "Interestingly, the veggie burger is more likely than the other two possible\n",
    "goals. This is because there are *more possible ways* to get the 5 ingredients\n",
    "required for the Greek salad or burrito bowl, whereas the user has to collect\n",
    "4 non-frozen ingredients for the veggie burger before obtaining the frozen\n",
    "veggie patty. As a result, if the user's goal was Greek salad, it's slightly\n",
    "less likely that they would happen to pick up the onion and tomato. After all,\n",
    "they could have easily picked up the olives or cucumber first!\n",
    "\n",
    "Now let's see what happens if the user get some rice after collecting the\n",
    "tomato and onion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(goal)\t\tgoal\n",
      "0.011\t\tgreek_salad\n",
      "0.017\t\tveggie_burger\n",
      "0.030\t\tfried_rice\n",
      "0.942\t\tburrito_bowl\n"
     ]
    }
   ],
   "source": [
    "observations = choicemap(\n",
    "    ((:act, 1), \"get(tomato)\"),\n",
    "    ((:act, 2), \"get(onion)\"),\n",
    "    ((:act, 3), \"get(rice)\"),\n",
    ")\n",
    "\n",
    "T = 3\n",
    "results = enum_inference(\n",
    "    act_only_user_model, (T,), observations, (:goal,), (GOALS,)\n",
    ")\n",
    "\n",
    "println(\"P(goal)\\t\\tgoal\")\n",
    "for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the user adds some rice to their shopping cart, it becomes obvious\n",
    "that they are shopping for a burrito bowl (assuming there are no other possible\n",
    "goals). The posterior over goals reflects this certainty.\n",
    "\n",
    "## 4. Modeling natural language instructions with LLMs <a name=\"modeling-instructions\"></a>\n",
    "\n",
    "At this point, we've built a model of how user acts over time to achieve their\n",
    "goal, then shown you how to infer the user's goal given a series of actions. Now\n",
    "we'll extend this with an *utterance model*, which models how a user might\n",
    "communicate their plan $\\pi$ at state $s_t$ as an instruction in\n",
    "natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utterance_model"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LLM mixture-of-prompts model\n",
    "gpt3_mixture =\n",
    "    GPT3Mixture(model=\"davinci-002\", stop=\"\\n\", max_tokens=512)\n",
    "\n",
    "# Few-shot examples of how commands are translated into instructions\n",
    "COMMAND_EXAMPLES = [\n",
    "    (\"get(apple)\", \"Can you get the apple?\"),\n",
    "    (\"get(bread)\", \"Could you find some bread?\"),\n",
    "    (\"get(cheddar_cheese)\", \"Go grab a block of that cheese.\"),\n",
    "    (\"get(green_tea)\", \"Add some tea to the cart.\"),\n",
    "    (\"checkout()\", \"Let's checkout.\"),\n",
    "    (\"get(tofu) get(seitan)\", \"I need some tofu and seitan.\"),\n",
    "    (\"get(frozen_mango) get(ice_cream)\", \"Get the mango and ice cream.\"),\n",
    "    (\"get(strawberries) get(milk)\", \"Find me strawberries and milk.\"),\n",
    "    (\"get(frozen_broccoli) get(frozen_cauliflower)\", \"We'll need frozen broccoli and cauliflower.\"),\n",
    "    (\"get(fries) checkout()\", \"Let's get some fries then checkout.\"),\n",
    "]\n",
    "Random.seed!(0)\n",
    "shuffle!(COMMAND_EXAMPLES)\n",
    "\n",
    "\"Construct few-shot prompt for translating a command into an instruction.\"\n",
    "function construct_utterance_prompt(\n",
    "    command::Vector{String}, examples = COMMAND_EXAMPLES\n",
    ")\n",
    "    example_strs = [\"Input: $cmd\\nOutput: $utt\" for (cmd, utt) in examples]\n",
    "    example_str = join(example_strs, \"\\n\")\n",
    "    command_str = join(command, \" \")\n",
    "    prompt = \"$example_str\\nInput: $command_str\\nOutput:\"\n",
    "    return prompt\n",
    "end\n",
    "\n",
    "\"Returns future planned actions in topologically sorted order.\"\n",
    "function get_future_actions(state::Set, plan::Dict)\n",
    "    future_acts = String[]\n",
    "    visited = Set{String}()\n",
    "    finished = Set{String}()\n",
    "    queue = collect(keys(plan))\n",
    "    while !isempty(queue)\n",
    "        act = queue[end]\n",
    "        if act in finished\n",
    "            pop!(queue)\n",
    "            continue\n",
    "        elseif act in visited\n",
    "            pop!(queue)\n",
    "            push!(finished, act)\n",
    "            act in state || push!(future_acts, act)\n",
    "        else\n",
    "            push!(visited, act)\n",
    "            for act_dep in plan[act]\n",
    "                act_dep in finished && continue\n",
    "                act_dep in visited && error(\"Cycle detected!\")\n",
    "                push!(queue, act_dep)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return future_acts\n",
    "end\n",
    "\n",
    "\"Model of how utterances are generated given the user's goal and plan.\"\n",
    "@gen function utterance_model(goal::String, plan::Dict, state::Set)\n",
    "    # Determine the set of future planned actions\n",
    "    future_acts = get_future_actions(state, plan)\n",
    "    # Enumerate all subsets of up to two actions as commands\n",
    "    commands = Vector{String}[]\n",
    "    for k in 1:2, acts in IterTools.subsets(future_acts, k)\n",
    "        push!(commands, collect(acts))\n",
    "    end\n",
    "    # Construct a prompt for each possible command\n",
    "    prompts = [construct_utterance_prompt(cmd) for cmd in commands]\n",
    "    # Generate an utterance from the LLM mixture-of-prompts model\n",
    "    utterance ~ gpt3_mixture(prompts)\n",
    "    return (utterance, commands)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our utterance model is a model of *goal-directed communication*: Given some plan\n",
    "$\\pi$ that the user has in mind, we:\n",
    "1. Select a random subset of the actions that have yet to be taken, forming\n",
    "   a *command* $c_t$: a code-like specification of the user's instruction.\n",
    "2. Translate this command into a natural language instruction $u_t$ using an LLM\n",
    "   (in this case, OpenAI's `davinci-002` model).\n",
    "\n",
    "How does the LLM perform this translation? Since LLMs are capable of in-context\n",
    "learning, all we have to provide is a list of few-shot examples\n",
    "(`COMMAND_EXAMPLES`) in the LLM's prompt. We then append the command we want\n",
    "to translate to the end of the prompt (`construct_utterance_prompt`),\n",
    "and generate a completion from the LLM.\n",
    "\n",
    "<details>\n",
    "<summary>Tell me more: Variance reduction via command enumeration.</summary><br>\n",
    "\n",
    "One thing you might notice about the utterance model is that we aren't actually\n",
    "sampling a random command $c_t$ to translate. Instead, we're enumerating over\n",
    "all possible commands, constructing a prompt for each, then passing them to a\n",
    "*mixture-of-prompts* model (`gpt3_mixture`). Under-the-hood, this mixture model\n",
    "samples a random prompt from the list of prompts, then generates a completion\n",
    "for that prompt. It then evaluates the total probability of the completion under\n",
    "*all* possible prompts.\n",
    "\n",
    "Why go through this process of enumerating all commands and their corresponding\n",
    "prompts, especially when it requires more calls to the LLM? The main reason\n",
    "is *variance reduction*: By using the mixture model, we can directly evaluate\n",
    "the likelihood $P(u_t | \\pi, g)$ of an utterance $u_t$ given the user's plan\n",
    "$\\pi$ and goal $g$. If we instead sampled a command $c_t ~ P(c_t | \\pi, g)$, we\n",
    "would get a noisy estimate $P(u_t| c_t, \\pi, g) \\approx P(u_t | \\pi, g)$ instead\n",
    "of the exact value. When the space of commands is small enough to enumerate\n",
    "over, this reduction in variance can often be worthwhile.\n",
    "</details><br>\n",
    "\n",
    "Let's see what utterance gets generated by our model when we specify the user's\n",
    "goal and plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance = \" Find me those yellow peas over there.\"\n"
     ]
    }
   ],
   "source": [
    "goal = \"fried_rice\"\n",
    "plan = PLANS[goal]\n",
    "state = Set{String}()\n",
    "utterance, commands = utterance_model(goal, plan, state)\n",
    "@show utterance;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't just want to generate instructions from an LLM. We want to\n",
    "*observe* them, then evaluate how likely that instruction is given the user's\n",
    "goal and plan. We can do this using the `Gen.generate` function we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likely_utterance_logprob = -21.43424906944275\n"
     ]
    }
   ],
   "source": [
    "# Specify utterance (with starting space to match OpenAI tokenization)\n",
    "utterance = \" We need soy sauce and onions.\"\n",
    "observations = choicemap((:utterance => :output, utterance))\n",
    "\n",
    "# Evaluate the log probability of the utterance\n",
    "trace, _ = Gen.generate(utterance_model, (goal, plan, state), observations)\n",
    "likely_utterance_logprob = Gen.get_score(trace)\n",
    "@show likely_utterance_logprob;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the (local) posterior distribution over commands\n",
    "$P(c_t | u_t, \\pi, g)$ that might have generated the observed instruction $u_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(command)\tcommand\n",
      "0.999\t\t[\"get(soy_sauce)\", \"get(onion)\"]\n",
      "0.000\t\t[\"get(soy_sauce)\"]\n",
      "0.000\t\t[\"get(soy_sauce)\", \"checkout()\"]\n",
      "0.000\t\t[\"get(soy_sauce)\", \"get(frozen_carrots)\"]\n",
      "0.000\t\t[\"get(soy_sauce)\", \"get(frozen_peas)\"]\n"
     ]
    }
   ],
   "source": [
    "# Show most likely commands, given the observed utterance\n",
    "commands = Gen.get_retval(trace)[2]\n",
    "command_probs = trace[:utterance => :post_probs]\n",
    "top_5_idxs = sortperm(command_probs, rev=true)[1:5]\n",
    "println(\"P(command)\\tcommand\")\n",
    "for idx in top_5_idxs\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", command_probs[idx], commands[idx])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we observe an utterance that is very unlikely given the user's\n",
    "plan (e.g. `\" Let's get some soap.\"`)? We should then expect the probability\n",
    "of the utterance $P(u_t | \\pi, g)$ to be lower. Furthermore, since no command\n",
    "$c_t$ can explain the utterance well, we should see a much more uncertain\n",
    "distribution $P(c_t | u_t, \\pi, g)$ over the possible commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlikely_utterance_logprob = -28.508935982328918\n",
      "P(command)\tcommand\n",
      "0.360\t\t[\"get(soy_sauce)\", \"checkout()\"]\n",
      "0.156\t\t[\"checkout()\"]\n",
      "0.068\t\t[\"get(rice)\", \"checkout()\"]\n",
      "0.061\t\t[\"get(onion)\", \"checkout()\"]\n",
      "0.057\t\t[\"get(soy_sauce)\", \"get(onion)\"]\n"
     ]
    }
   ],
   "source": [
    "# Specify unlikely utterance (with starting space to match OpenAI tokenization)\n",
    "utterance = \" Let's get some soap.\"\n",
    "observations = choicemap((:utterance => :output, utterance))\n",
    "\n",
    "# Evaluate the log probability of the unlikely utterance\n",
    "trace, _ = Gen.generate(utterance_model, (goal, plan, state), observations)\n",
    "unlikely_utterance_logprob = Gen.get_score(trace)\n",
    "@show unlikely_utterance_logprob\n",
    "\n",
    "# Show most likely commands, given the observed utterance\n",
    "commands = Gen.get_retval(trace)[2]\n",
    "command_probs = trace[:utterance => :post_probs]\n",
    "top_5_idxs = sortperm(command_probs, rev=true)[1:5]\n",
    "println(\"P(command)\\tcommand\")\n",
    "for idx in top_5_idxs\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", command_probs[idx], commands[idx])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inferring goals from actions and instructions <a name=\"goals-from-instructions\"></a>\n",
    "\n",
    "So far we've seen how to evaluate the likelihood $P(u_t | \\pi, g)$ of an\n",
    "utterance $u_t$ given a known plan $\\pi$ and goal $g$, and to compute the\n",
    "*local* posterior distribution over commands $P(c_t | u_t, \\pi, g)$.\n",
    "\n",
    "In general, however, we don't know the user's goal $g$ or plan $\\pi$. To\n",
    "model this situation, we need to embed `utterance_model` as a *sub-routine*\n",
    "within our full user model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_user_model"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Model of user's goal-directed actions and instructions over time.\"\n",
    "@gen function full_user_model(T::Int, act_noise::Real = 0.05)\n",
    "    # Construct initial state of environment\n",
    "    state = Set{String}()\n",
    "    # Sample user's goal and select plan\n",
    "    goal ~ labeled_uniform(GOALS)\n",
    "    plan = PLANS[goal]\n",
    "    # Decide whether to speak at the beginning\n",
    "    speak = {(:speak, 0)} ~ bernoulli(0.2)\n",
    "    # Generate utterance that communicates the current goal and plan\n",
    "    if speak\n",
    "        {(:utterance, 0)} ~ utterance_model(goal, plan, state)\n",
    "    end\n",
    "    # Sample actions and utterances at each timestep\n",
    "    act_history = String[]\n",
    "    for t in 1:T\n",
    "        # Determine user's next possible actions\n",
    "        planned_acts = get_planned_actions(state, plan)\n",
    "        planned_probs = fill((1.0 - act_noise) / length(planned_acts),\n",
    "                             length(planned_acts))\n",
    "        # Determine set of unexecuted actions\n",
    "        possible_acts = filter(!in(state), ACTIONS)\n",
    "        possible_probs = fill(act_noise / length(possible_acts),\n",
    "                              length(possible_acts))\n",
    "        # Sample next action (with some action noise)\n",
    "        next_acts = vcat(planned_acts, possible_acts)\n",
    "        next_act_probs = vcat(planned_probs, possible_probs)\n",
    "        act = {(:act, t)} ~ labeled_categorical(next_acts, next_act_probs)\n",
    "        # Update state and action history\n",
    "        if act != \"wait()\"\n",
    "            push!(state, act)\n",
    "        end\n",
    "        push!(act_history, act)\n",
    "        # Decide whether to speak at this timestep\n",
    "        speak = {(:speak, t)} ~ bernoulli(0.2)\n",
    "        # Generate utterance that communicates the current goal and plan\n",
    "        if speak\n",
    "            {(:utterance, t)} ~ utterance_model(goal, plan, state)\n",
    "        end\n",
    "    end\n",
    "    # Return final state and action history\n",
    "    return (state, act_history)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model extends `act_only_user_model` by possibly generating an utterance at\n",
    "each step $t$. This is done by first sampling whether or not the user speaks\n",
    "(denoted by the address `(:speak, t)`), then generating an utterance from the\n",
    "`utterance_model` if the user decides to speak.\n",
    "\n",
    "Since this model describes how a user might act and talk given a particular\n",
    "goal, we can condition on both actions and instructions to infer their goal:\n",
    "\n",
    "**Note: Latency of the following code may be high since we are making multiple\n",
    "requests to OpenAI's LLM API over the web, instead of running an LLM locally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(goal)\t\tgoal\n",
      "0.303\t\tgreek_salad\n",
      "0.393\t\tveggie_burger\n",
      "0.000\t\tfried_rice\n",
      "0.305\t\tburrito_bowl\n"
     ]
    }
   ],
   "source": [
    "# Observed actions and instructions\n",
    "observations = choicemap(\n",
    "    ((:speak, 0), true),\n",
    "    ((:utterance, 0) => :utterance => :output, \" Can you grab a tomato?\"),\n",
    "    ((:act, 1), \"get(onion)\"), ((:speak, 1), false),\n",
    ")\n",
    "\n",
    "# Run inference by enumerating over all possible goals\n",
    "T = 1\n",
    "results = enum_inference(\n",
    "    full_user_model, (T,), observations, (:goal,), (GOALS,)\n",
    ")\n",
    "\n",
    "# Show inferred goal probabilities\n",
    "println(\"P(goal)\\t\\tgoal\")\n",
    "for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we first observe the user say `\" Can you grab a tomato?\"`\n",
    "at $t=0$. At $t=1$, the user then takes the action `get(onion)`. Bayesian\n",
    "inference automatically combines these pieces of information, inferring once\n",
    "again that the user might be shopping for a Greek salad, veggier burger, or\n",
    "burrito bowl.\n",
    "\n",
    "In addition to inferring the user's goal $g$, we can infer the command $c_t$\n",
    "that led to the instruction $u_t$. We do this by summing the local command\n",
    "posteriors $P(c_t | u_t, \\pi, g)$ across possible plans $\\pi$ and goals $g$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(command)\tcommand\n",
      "0.817\t\t[\"get(tomato)\"]\n",
      "0.159\t\t[\"get(tomato)\", \"checkout()\"]\n",
      "0.010\t\t[\"get(tomato)\", \"get(frozen_patty)\"]\n",
      "0.007\t\t[\"get(tomato)\", \"get(hamburger_bun)\"]\n",
      "0.002\t\t[\"get(tomato)\", \"get(rice)\"]\n"
     ]
    }
   ],
   "source": [
    "\"Extract posterior over commands at step `t` given a list of weighted traces.\"\n",
    "function extract_command_probs(\n",
    "    t::Int, traces::AbstractVector{<:Trace}, logprobs::AbstractVector{<:Real}\n",
    ")\n",
    "    # Sum local command posteriors across traces\n",
    "    command_probs = Dict{Vector{String}, Float64}()\n",
    "    log_total = logsumexp(logprobs)\n",
    "    for (tr, lp) in zip(traces, logprobs)\n",
    "        _, trace_commands = tr[(:utterance, t)]\n",
    "        trace_command_probs = tr[(:utterance, t) => :utterance => :post_probs]\n",
    "        trace_prob = exp(lp - log_total)\n",
    "        for (cmd, p) in zip(trace_commands, trace_command_probs)\n",
    "            command_probs[cmd] = get(command_probs, cmd, 0.0) + p * trace_prob\n",
    "        end\n",
    "    end\n",
    "    # Sort commands by probability\n",
    "    commands = collect(keys(command_probs))\n",
    "    command_probs = collect(values(command_probs))\n",
    "    idxs = sortperm(command_probs, rev=true)\n",
    "    return commands[idxs], command_probs[idxs]\n",
    "end\n",
    "\n",
    "# Show top inferred command probabilities\n",
    "commands, command_probs = extract_command_probs(0, results.traces, results.logprobs)\n",
    "println(\"P(command)\\tcommand\")\n",
    "for (cmd, prob) in zip(commands[1:5], command_probs[1:5])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, cmd)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the utterance `\" Can you grab a tomato?\"` is most likely to have\n",
    "been generated from the command `get(tomato)`.\n",
    "\n",
    "The ability to combine goal-relevant information across modalities means that\n",
    "CLIPS can *disambiguate* instructions that would be ambiguous without context.\n",
    "Imagine that the user says `\"Can you get the stuff in the frozen section?\"`\n",
    "at $t=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(goal)\t\tgoal\n",
      "0.007\t\tgreek_salad\n",
      "0.419\t\tveggie_burger\n",
      "0.562\t\tfried_rice\n",
      "0.012\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "0.188\t\t[\"get(frozen_patty)\"]\n",
      "0.127\t\t[\"get(frozen_carrots)\", \"checkout()\"]\n",
      "0.126\t\t[\"get(frozen_carrots)\"]\n",
      "0.104\t\t[\"get(frozen_peas)\", \"get(frozen_carrots)\"]\n",
      "0.086\t\t[\"get(tomato)\", \"get(frozen_patty)\"]\n"
     ]
    }
   ],
   "source": [
    "observations = choicemap(\n",
    "    ((:speak, 0), true),\n",
    "    ((:utterance, 0) => :utterance => :output,\n",
    "     \" Can you get the stuff in the frozen section?\"),\n",
    ")\n",
    "\n",
    "T = 0\n",
    "results = enum_inference(\n",
    "    full_user_model, (T,), observations, (:goal,), (GOALS,)\n",
    ")\n",
    "\n",
    "println(\"P(goal)\\t\\tgoal\")\n",
    "for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "end\n",
    "println()\n",
    "\n",
    "commands, command_probs = extract_command_probs(0, results.traces, results.logprobs)\n",
    "println(\"P(command)\\tcommand\")\n",
    "for (cmd, prob) in zip(commands[1:5], command_probs[1:5])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, cmd)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited information, it's not clear if the user wants the frozen\n",
    "patty (for the veggie burger), or the frozen peas and carrots (for fried rice).\n",
    "As a result, CLIPS assigns about equal probability to each of those goals.\n",
    "\n",
    "However, if we see the user get some rice before asking for the frozen stuff,\n",
    "their intentions become much clearer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(goal)\t\tgoal\n",
      "0.000\t\tgreek_salad\n",
      "0.005\t\tveggie_burger\n",
      "0.985\t\tfried_rice\n",
      "0.010\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "0.235\t\t[\"get(frozen_carrots)\"]\n",
      "0.232\t\t[\"get(frozen_carrots)\", \"checkout()\"]\n",
      "0.219\t\t[\"get(frozen_peas)\", \"get(frozen_carrots)\"]\n",
      "0.080\t\t[\"get(onion)\", \"get(frozen_carrots)\"]\n",
      "0.061\t\t[\"get(frozen_peas)\"]\n"
     ]
    }
   ],
   "source": [
    "observations = choicemap(\n",
    "    ((:speak, 0), false),\n",
    "    ((:act, 1), \"get(rice)\"), ((:speak, 1), true),\n",
    "    ((:utterance, 1) => :utterance => :output,\n",
    "     \" Can you get the stuff in the frozen section?\"),\n",
    ")\n",
    "\n",
    "T = 1\n",
    "results = enum_inference(\n",
    "    full_user_model, (T,), observations, (:goal,), (GOALS,)\n",
    ")\n",
    "\n",
    "println(\"P(goal)\\t\\tgoal\")\n",
    "for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "end\n",
    "println()\n",
    "\n",
    "commands, command_probs = extract_command_probs(1, results.traces, results.logprobs)\n",
    "println(\"P(command)\\tcommand\")\n",
    "for (cmd, prob) in zip(commands[1:5], command_probs[1:5])\n",
    "    @printf(\"%.3f\\t\\t%s\\n\", prob, cmd)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modifying the code above to see how CLIPS handles ambiguous instructions\n",
    "in the presence or absence of actions. Here are some possibilities:\n",
    "- `\" I need to get some veggies.\"`\n",
    "- `\" Could you help find the cheese?\"`\n",
    "- `\" Let's get the last ingredient then checkout.\"`\n",
    "- `\" We need to get the fresh stuff first.\"`\n",
    "\n",
    "For each of these cases, the instruction alone is not enough to determine the\n",
    "user's true goal. Actions are required to disambiguate the user's intentions.\n",
    "\n",
    "## 6. Interactive user assistance <a name=\"user-assistance\"></a>\n",
    "\n",
    "We've now seen how to model a user's actions and instructions over time, and how\n",
    "to infer their goal and commands given those actions and instructions. All of\n",
    "this is still *passive*, however. In this section, we'll show you how to write\n",
    "an *interactive assistant* that takes actions based on what it infers about\n",
    "the user at each step $t$.\n",
    "\n",
    "To do this, we'll need to adjust our user model so that it takes a list of\n",
    "*assistant actions* as an argument. At each step $t$, the assistant either\n",
    "does nothing or takes an action. If the assistant does nothing, then the user\n",
    "will act; otherwise, only the assistant will act. The user may also give\n",
    "instructions at any step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive_user_model"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Model of a user interacting with an assistant over time.\"\n",
    "@gen function interactive_user_model(\n",
    "    assist_actions::AbstractVector, act_noise::Real = 0.05\n",
    ")\n",
    "    # Construct initial state of environment\n",
    "    state = Set{String}()\n",
    "    # Sample user's goal and select plan\n",
    "    goal ~ labeled_uniform(GOALS)\n",
    "    plan = PLANS[goal]\n",
    "    # Decide whether to speak at the beginning\n",
    "    speak = {(:speak, 0)} ~ bernoulli(0.2)\n",
    "    # Generate utterance that communicates the current goal and plan\n",
    "    if speak\n",
    "        {(:utterance, 0)} ~ utterance_model(goal, plan, state)\n",
    "    end\n",
    "    # Sample actions and utterances at each timestep\n",
    "    act_history = String[]\n",
    "    for (t, assist_act) in enumerate(assist_actions)\n",
    "        if isnothing(assist_act) # User acts, assistant does not\n",
    "            # Determine user's next possible actions\n",
    "            planned_acts = get_planned_actions(state, plan)\n",
    "            planned_probs = fill((1.0 - act_noise) / length(planned_acts),\n",
    "                                 length(planned_acts))\n",
    "            # Determine set of unexecuted actions\n",
    "            possible_acts = filter(!in(state), ACTIONS)\n",
    "            possible_probs = fill(act_noise / length(possible_acts),\n",
    "                                  length(possible_acts))\n",
    "            # Construct action distribution\n",
    "            next_acts = vcat(planned_acts, possible_acts)\n",
    "            next_act_probs = vcat(planned_probs, possible_probs)\n",
    "        else # Assistant acts, user does not\n",
    "            @assert assist_act in ACTIONS\n",
    "            next_acts = [assist_act]\n",
    "            next_act_probs = [1.0]\n",
    "        end\n",
    "        # Sample next action\n",
    "        act = {(:act, t)} ~ labeled_categorical(next_acts, next_act_probs)\n",
    "        # Update state and action history\n",
    "        if act != \"wait()\"\n",
    "            push!(state, act)\n",
    "        end\n",
    "        push!(act_history, act)\n",
    "        # Decide whether to speak at this timestep\n",
    "        speak = {(:speak, t)} ~ bernoulli(0.2)\n",
    "        # Generate utterance that communicates the current goal and plan\n",
    "        if speak\n",
    "            {(:utterance, t)} ~ utterance_model(goal, plan, state)\n",
    "        end\n",
    "    end\n",
    "    # Return final state and action history\n",
    "    return (state, act_history)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also extend our enumerative inference algorithm so that we can update\n",
    "our inferences step-by-step, instead of having to observe the user's actions\n",
    "all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enum_inference_step"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    enum_inference_step(prev_results, new_model_args, new_observations)\n",
    "\n",
    "Updates a set of inference results (`prev_results`) by adusting the model's\n",
    "arguments to `new_model_args`, and conditioning on `new_observations`.\n",
    "\"\"\"\n",
    "function enum_inference_step(\n",
    "    prev_results::NamedTuple, new_model_args::Tuple, new_observations::ChoiceMap\n",
    ")\n",
    "    # Update previous traces with the new arguments and observations\n",
    "    argdiffs = map(_ -> UnknownChange(), new_model_args)\n",
    "    traces = map(prev_results.traces) do prev_trace\n",
    "        trace, _, _, _ =\n",
    "            Gen.update(prev_trace, new_model_args, argdiffs, new_observations)\n",
    "        return trace\n",
    "    end\n",
    "    # Compute the log probability of each trace\n",
    "    logprobs = map(Gen.get_score, traces)\n",
    "    # Compute the log marginal likelihood of the observations\n",
    "    lml = logsumexp(logprobs)\n",
    "    # Compute the (marginal) posterior probabilities for each latent variable\n",
    "    latent_logprobs = Dict(\n",
    "        addr => ([logsumexp(lps) for lps in eachslice(logprobs, dims=i)] .- lml)\n",
    "        for (i, addr) in enumerate(prev_results.latent_addrs)\n",
    "    )\n",
    "    latent_probs = Dict(addr => exp.(lp) for (addr, lp) in latent_logprobs)\n",
    "    return (\n",
    "        traces = traces,\n",
    "        logprobs = logprobs,\n",
    "        latent_logprobs = latent_logprobs,\n",
    "        latent_probs = latent_probs,\n",
    "        latent_addrs = prev_results.latent_addrs,\n",
    "        lml = lml\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define an assistance policy that takes in the observation history\n",
    "and inferences about the user, then returns an action. We'll take a relatively\n",
    "conservative approach where:\n",
    "\n",
    "- The assistant only acts after the user has spoken / given an instruction.\n",
    "- If the user's goal or command is too uncertain, the assistant does nothing.\n",
    "- Otherwise, the assistant follows the inferred command by taking\n",
    "  the most likely uncompleted action that is part of the command.\n",
    "- If all commanded actions are completed, the assistant does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assistance_policy"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    assistance_policy(t, state, observations, results)\n",
    "\n",
    "Returns an assistive action given the current step `t`, environment `state`,\n",
    "history of `observations`, and inferences about the user (`results`).\n",
    "\"\"\"\n",
    "function assistance_policy(\n",
    "    t::Int, state::Set, observations::ChoiceMap, results::NamedTuple;\n",
    "    goal_thresh::Real = 0.25, cmd_thresh::Real = 0.5\n",
    ")\n",
    "    # Find most recent instruction\n",
    "    t_speak = nothing\n",
    "    for i in t:-1:0\n",
    "        Gen.has_value(observations, (:speak, i)) || continue\n",
    "        observations[(:speak, i)] == true || continue\n",
    "        t_speak = i\n",
    "        break\n",
    "    end\n",
    "    # Do nothing if user has not spoken\n",
    "    isnothing(t_speak) && return nothing\n",
    "    # Extract current posterior over goals and most recent command\n",
    "    goal_probs = results.latent_probs[:goal]\n",
    "    commands, command_probs =\n",
    "        extract_command_probs(t_speak, results.traces, results.logprobs)\n",
    "    # Determine most likely action from command distribution\n",
    "    act_probs = Dict{String,Float64}()\n",
    "    for (cmd, prob) in zip(commands, command_probs), act in cmd\n",
    "        act in state && continue # Ignore completed actions\n",
    "        act_probs[act] = get!(act_probs, act, 0.0) + prob\n",
    "    end\n",
    "    isempty(act_probs) && return nothing\n",
    "    max_act_prob, max_act = findmax(act_probs)\n",
    "    # Do nothing if user's goal or command is too uncertain\n",
    "    maximum(goal_probs) < goal_thresh && return nothing\n",
    "    max_act_prob < cmd_thresh && return nothing\n",
    "    # Take the most likely uncompleted action\n",
    "    return max_act\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is only one way to design an assistance policy. In the full\n",
    "version of CLIPS, we implemented a more pro-active form of assistance which acts\n",
    "at every step to minimize the expected cost of achieving the user's goal.\n",
    "While this can be more helpful, it can also provide help even in cases where the\n",
    "user has not requested for help, and may not even want it. The policy shown\n",
    "above avoids such situations.\n",
    "\n",
    "Now let's put everything together into a Read-Evaluate-Print Loop (REPL). To\n",
    "make things more fun, we'll turn this into a game, where:\n",
    "- You, the user, will be given a goal to achieve.\n",
    "- At each step, you can either act or give an instruction to the assistant.\n",
    "- The assistant will respond if it's confident enough about your intentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clips_repl"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"REPL-based assistance game with a CLIPS assistance policy.\"\n",
    "function clips_repl(\n",
    "    max_steps::Int = 15;\n",
    "    act_noise::Real = 0.05, goal_thresh::Real = 0.25, cmd_thresh::Real = 0.5\n",
    ")\n",
    "    assist_actions = Union{String, Nothing}[]\n",
    "    act_history = String[]\n",
    "    state = Set{String}()\n",
    "    # Sample a random goal for the user to achieve\n",
    "    true_goal = rand(GOALS)\n",
    "    # Construct initial observation choicemap (no speaking at t = 0)\n",
    "    utterance = \"\"\n",
    "    observations = choicemap(((:speak, 0), false))\n",
    "    # Initialize goal and command inferences\n",
    "    results = enum_inference(\n",
    "        interactive_user_model, (assist_actions, act_noise), observations,\n",
    "        (:goal,), (GOALS,)\n",
    "    )\n",
    "    # Loop up to maximum number of steps\n",
    "    for t in 1:max_steps\n",
    "        if !isempty(assist_actions) && isnothing(assist_actions[end])\n",
    "            # Show inferred goal probabilities\n",
    "            println(\"P(goal)\\t\\tgoal\")\n",
    "            println(\"-\"^50)\n",
    "            for (goal, prob) in zip(GOALS, results.latent_probs[:goal])\n",
    "                @printf(\"%.3f\\t\\t%s\\n\", prob, goal)\n",
    "            end\n",
    "            println()\n",
    "            # Show inferred command probabilities\n",
    "            if !isempty(utterance)\n",
    "                commands, command_probs =\n",
    "                    extract_command_probs(t-1, results.traces, results.logprobs)\n",
    "                println(\"P(command)\\tcommand\")\n",
    "                println(\"-\"^80)\n",
    "                for (cmd, prob) in zip(commands[1:5], command_probs[1:5])\n",
    "                    @printf(\"%.3f\\t\\t%s\\n\", prob, cmd)\n",
    "                end\n",
    "                println()\n",
    "            end\n",
    "        end\n",
    "        # Decide whether to act based on assistance policy\n",
    "        assist_act = assistance_policy(t-1, state, observations, results;\n",
    "                                       goal_thresh, cmd_thresh)\n",
    "        push!(assist_actions, assist_act)\n",
    "        println(\"=== t = $t ===\")\n",
    "        # Request user's next input if assistant does not act\n",
    "        if isnothing(assist_act)\n",
    "            possible_acts = filter(!in(state), ACTIONS)\n",
    "            remaining_acts = get_future_actions(state, PLANS[true_goal])\n",
    "            println(\"Goal: $true_goal\", \"\\n\")\n",
    "            println(\"Remaining Actions: \", join(remaining_acts, \" \"), \"\\n\")\n",
    "            println(\"Possible Actions: \", join(possible_acts, \" \"), \"\\n\")\n",
    "            print(\"User: \")\n",
    "            input = strip(readline())\n",
    "            if input in possible_acts # User takes an action\n",
    "                act = input\n",
    "                utterance = \"\"\n",
    "            else # User gives an instruction\n",
    "                act = \"wait()\"\n",
    "                utterance = input\n",
    "            end\n",
    "        else\n",
    "            act = assist_act\n",
    "            utterance = \"\"\n",
    "            println(\"Assistant: \", act)\n",
    "        end\n",
    "        # Update environment state\n",
    "        push!(act_history, act)\n",
    "        if act != \"wait()\"\n",
    "            push!(state, act)\n",
    "        end\n",
    "        # Check if the goal has been achieved\n",
    "        if keys(PLANS[true_goal]) ⊆ state\n",
    "            println(\"\\n=== Goal achieved: $true_goal ===\\n\")\n",
    "            return (state, act_history, assist_actions)\n",
    "        end\n",
    "        # Construct next observation choicemap\n",
    "        if !isempty(utterance)\n",
    "            new_obs = choicemap(\n",
    "                ((:speak, t), true), ((:act, t), act),\n",
    "                ((:utterance, t) => :utterance => :output, \" \" * utterance),\n",
    "            )\n",
    "        else\n",
    "            new_obs = choicemap(((:speak, t), false), ((:act, t), act))\n",
    "        end\n",
    "        observations = merge(observations, new_obs)\n",
    "        # Update goal and command inferences\n",
    "        if !isempty(utterance)\n",
    "            println(\"\\nRunning inference (LLM queries may take a while)...\\n\")\n",
    "        elseif isnothing(assist_act)\n",
    "            println(\"\\nRunning inference...\\n\")\n",
    "        else\n",
    "            println()\n",
    "        end\n",
    "        new_args = (assist_actions, act_noise)\n",
    "        results = enum_inference_step(results, new_args, new_obs)\n",
    "    end\n",
    "    return (state, act_history, assist_actions)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now interact with the CLIPS assistance policy via the REPL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== t = 1 ===\n",
      "Goal: fried_rice\n",
      "\n",
      "Remaining Actions: get(soy_sauce) get(onion) get(rice) get(frozen_peas) get(frozen_carrots) checkout()\n",
      "\n",
      "Possible Actions: checkout() get(black_beans) get(cotija_cheese) get(cucumber) get(feta_cheese) get(frozen_carrots) get(frozen_patty) get(frozen_peas) get(hamburger_bun) get(lettuce) get(olives) get(onion) get(rice) get(soy_sauce) get(tomato) wait()\n",
      "\n",
      "User: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin>  Hello assistant! How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference (LLM queries may take a while)...\n",
      "\n",
      "P(goal)\t\tgoal\n",
      "--------------------------------------------------\n",
      "0.219\t\tgreek_salad\n",
      "0.174\t\tveggie_burger\n",
      "0.285\t\tfried_rice\n",
      "0.321\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "--------------------------------------------------------------------------------\n",
      "0.138\t\t[\"checkout()\"]\n",
      "0.096\t\t[\"get(cotija_cheese)\"]\n",
      "0.084\t\t[\"get(tomato)\"]\n",
      "0.052\t\t[\"get(frozen_carrots)\"]\n",
      "0.050\t\t[\"get(onion)\"]\n",
      "\n",
      "=== t = 2 ===\n",
      "Goal: fried_rice\n",
      "\n",
      "Remaining Actions: get(soy_sauce) get(onion) get(rice) get(frozen_peas) get(frozen_carrots) checkout()\n",
      "\n",
      "Possible Actions: checkout() get(black_beans) get(cotija_cheese) get(cucumber) get(feta_cheese) get(frozen_carrots) get(frozen_patty) get(frozen_peas) get(hamburger_bun) get(lettuce) get(olives) get(onion) get(rice) get(soy_sauce) get(tomato) wait()\n",
      "\n",
      "User: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin>  get(rice)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference...\n",
      "\n",
      "P(goal)\t\tgoal\n",
      "--------------------------------------------------\n",
      "0.004\t\tgreek_salad\n",
      "0.004\t\tveggie_burger\n",
      "0.590\t\tfried_rice\n",
      "0.402\t\tburrito_bowl\n",
      "\n",
      "=== t = 3 ===\n",
      "Goal: fried_rice\n",
      "\n",
      "Remaining Actions: get(soy_sauce) get(onion) get(frozen_peas) get(frozen_carrots) checkout()\n",
      "\n",
      "Possible Actions: checkout() get(black_beans) get(cotija_cheese) get(cucumber) get(feta_cheese) get(frozen_carrots) get(frozen_patty) get(frozen_peas) get(hamburger_bun) get(lettuce) get(olives) get(onion) get(soy_sauce) get(tomato) wait()\n",
      "\n",
      "User: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin>  Can you get the sauce and onion?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference (LLM queries may take a while)...\n",
      "\n",
      "P(goal)\t\tgoal\n",
      "--------------------------------------------------\n",
      "0.000\t\tgreek_salad\n",
      "0.000\t\tveggie_burger\n",
      "0.999\t\tfried_rice\n",
      "0.001\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "--------------------------------------------------------------------------------\n",
      "0.998\t\t[\"get(soy_sauce)\", \"get(onion)\"]\n",
      "0.001\t\t[\"get(cotija_cheese)\", \"get(onion)\"]\n",
      "0.000\t\t[\"get(tomato)\", \"get(onion)\"]\n",
      "0.000\t\t[\"get(onion)\", \"checkout()\"]\n",
      "0.000\t\t[\"get(onion)\"]\n",
      "\n",
      "=== t = 4 ===\n",
      "Assistant: get(onion)\n",
      "\n",
      "=== t = 5 ===\n",
      "Assistant: get(soy_sauce)\n",
      "\n",
      "=== t = 6 ===\n",
      "Goal: fried_rice\n",
      "\n",
      "Remaining Actions: get(frozen_peas) get(frozen_carrots) checkout()\n",
      "\n",
      "Possible Actions: checkout() get(black_beans) get(cotija_cheese) get(cucumber) get(feta_cheese) get(frozen_carrots) get(frozen_patty) get(frozen_peas) get(hamburger_bun) get(lettuce) get(olives) get(tomato) wait()\n",
      "\n",
      "User: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin>  Now get the frozen veggies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference (LLM queries may take a while)...\n",
      "\n",
      "P(goal)\t\tgoal\n",
      "--------------------------------------------------\n",
      "0.000\t\tgreek_salad\n",
      "0.000\t\tveggie_burger\n",
      "1.000\t\tfried_rice\n",
      "0.000\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "--------------------------------------------------------------------------------\n",
      "0.400\t\t[\"get(frozen_peas)\", \"get(frozen_carrots)\"]\n",
      "0.306\t\t[\"get(frozen_carrots)\"]\n",
      "0.179\t\t[\"get(frozen_carrots)\", \"checkout()\"]\n",
      "0.076\t\t[\"get(frozen_peas)\"]\n",
      "0.028\t\t[\"get(frozen_peas)\", \"checkout()\"]\n",
      "\n",
      "=== t = 7 ===\n",
      "Assistant: get(frozen_carrots)\n",
      "\n",
      "=== t = 8 ===\n",
      "Assistant: get(frozen_peas)\n",
      "\n",
      "=== t = 9 ===\n",
      "Goal: fried_rice\n",
      "\n",
      "Remaining Actions: checkout()\n",
      "\n",
      "Possible Actions: checkout() get(black_beans) get(cotija_cheese) get(cucumber) get(feta_cheese) get(frozen_patty) get(hamburger_bun) get(lettuce) get(olives) get(tomato) wait()\n",
      "\n",
      "User: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin>  Alright, let's checkout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference (LLM queries may take a while)...\n",
      "\n",
      "P(goal)\t\tgoal\n",
      "--------------------------------------------------\n",
      "0.000\t\tgreek_salad\n",
      "0.000\t\tveggie_burger\n",
      "1.000\t\tfried_rice\n",
      "0.000\t\tburrito_bowl\n",
      "\n",
      "P(command)\tcommand\n",
      "--------------------------------------------------------------------------------\n",
      "1.000\t\t[\"checkout()\"]\n",
      "0.000\t\t[\"get(black_beans)\", \"checkout()\"]\n",
      "0.000\t\t[\"get(cotija_cheese)\", \"checkout()\"]\n",
      "0.000\t\t[\"get(tomato)\", \"checkout()\"]\n",
      "0.000\t\t[\"get(lettuce)\", \"checkout()\"]\n",
      "\n",
      "=== t = 10 ===\n",
      "Assistant: checkout()\n",
      "\n",
      "=== Goal achieved: fried_rice ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Set([\"get(rice)\", \"get(frozen_carrots)\", \"get(onion)\", \"checkout()\", \"get(frozen_peas)\", \"get(soy_sauce)\"]), [\"wait()\", \"get(rice)\", \"wait()\", \"get(onion)\", \"get(soy_sauce)\", \"wait()\", \"get(frozen_carrots)\", \"get(frozen_peas)\", \"wait()\", \"checkout()\"], Union{Nothing, String}[nothing, nothing, nothing, \"get(onion)\", \"get(soy_sauce)\", nothing, \"get(frozen_carrots)\", \"get(frozen_peas)\", nothing, \"checkout()\"])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(state, act_history, assist_actions) = clips_repl(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CLIPS assistant is quite accurate in its ability to respond\n",
    "to ambiguous instructions, while still avoiding actions when the instructions\n",
    "are very unclear. CLIPS also flexibly handles user input, whether it takes\n",
    "the form of actions or free-form text. This can easily be extended to a non-REPL\n",
    "context, where users' take actions via UI elements in an app or video game.\n",
    "\n",
    "## 7. Possible extensions <a name=\"extensions\"></a>\n",
    "\n",
    "Since the design of CLIPS is highly modular and interpretable, it's not hard to\n",
    "add new features by enriching either the user model, the utterance model, or the\n",
    "assistance policy. For example, you could expand the user model with:\n",
    "- A larger space of possible goals, perhaps by sampling from an LLM.\n",
    "- [Automatic plan generation](https://github.com/JuliaPlanners/SymbolicPlanners.jl) given a goal (as in the full version of CLIPS).\n",
    "\n",
    "The utterance model could also support more types of utterances, such as:\n",
    "- Utterances which directly inform the assistant about the goal.\n",
    "- Instructions which refer back to an earlier instruction or assistant's action.\n",
    "- Explicit modeling of outlier or adversarial instructions to improve robustness.\n",
    "\n",
    "The assistance policy could be extended by:\n",
    "- More pro-active forms of assistance (as in the full version of CLIPS).\n",
    "- Asking of clarification questions in response to unclear instructions.\n",
    "\n",
    "Finally, to reduce LLM usage and scale CLIPS to larger domains, some\n",
    "possibilities include:\n",
    "- Using smaller local LLMs which are fine-tuned on domain-relevant instructions.\n",
    "- When translating commands to utterances, automatically retrieving similar\n",
    "  few-shot examples to include in the LLM prompt.\n",
    "- Instead of enumerating over all possible commands given a plan, inferring\n",
    "  commands bottom-up from the utterance $u_t$, using constrained generation\n",
    "  methods such as [Sequential Monte Carlo (SMC) steering](https://github.com/probcomp/hfppl)\n",
    "\n",
    "That's it for this tutorial — happy experimenting with CLIPS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
